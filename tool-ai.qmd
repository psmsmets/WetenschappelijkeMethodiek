# Generatieve AI en LLMs {#sec-ai}

Gen-AI of voluit *Generatieve Artificiële Intelligentie* is een verzamelnaam voor systemen die zelf nieuwe inhoud kunnen creëren, zoals tekst, beeld, muziek of video. De bekendste voorbeelden zijn taalmodellen, bv. ChatGPT, die teksten genereren op basis van eerdere patronen in de data waarop ze getraind zijn. 

Een *Large Language Model* (LLM) vormt de kern van veel van deze systemen. Zo’n model is eigenlijk een *voorspellingsmachine*: het probeert telkens het meest waarschijnlijke volgende woord te kiezen, gegeven de woorden die al zijn geschreven. Die voorspelling gebeurt via een neuraal netwerk dat miljoenen tot miljarden parameters bevat. Hoewel dit enorm krachtig is, blijft het principe verrassend eenvoudig: het is grotendeels het oplossen van zeer veel eerstegraadsvergelijkingen, statistiek en kansrekening.

In feite is er van intelligentie geen sprake, een taalmodel kan namelijk niet denken of redeneren. Een taalmodel is eerder een extreem goede patroonherkenner.

Het resultaat is een krachtige technologie die vloeiende en creatieve teksten kan produceren, maar die tegelijk kritisch gebruik en controle vereist. Voor onderwijs betekent dit: altijd nagaan of de output klopt, bronnen checken, en leren om de sterktes en beperkingen van AI in te schatten.

::: {.callout-note title="Welke AI model?"}
De bekendste AI-modellen zijn onder meer [ChatGPT](https://chatgpt.com), [Claude](https://claude.ai), [Copilot](https://copilot.microsoft.com), [Gemini](https://gemini.google.com) en [Perplexity](https://perplexity.ai).

Het meest ideale model voor onderwijs (veilig, transparant en zonder vendor lock-in) is **[Le Chat](https://chat.mistral.ai)**, een AI-model ontwikkeld door het Franse Mistral AI en ontworpen met focus op Europese privacywaarden en GDPR-compliance. Gebruikers kunnen zonder account werken en hebben de optie om modeltraining met hun data uit te schakelen (*opt-out* via de instellingen bovenaan).
:::

## Beknopte werking

### Tokens – de bouwstenen

Een tekst wordt voor het model opgedeeld in *tokens*. Dat zijn kleine stukjes tekst, soms een volledig woord en soms slechts een deel ervan. Elke token heeft bovendien een eigen numerieke ID.

Zo wordt bijvoorbeeld het woord *automobiliteit* bij GPT-4o opgesplitst in de tokens `auto`, `mobil`, `iteit` met IDs `[24028, 10427, 13244]`. 
Het model werkt dus niet met "woorden" zoals mensen dat doen, maar met reeksen tokens. 
De token IDs vormen het vocabulair van het LLM. 
Door telkens het volgende token te voorspellen, ontstaat een volledige zin of paragraaf.

::: {.callout-tip}
## OpenAI Tokenizer

Gebruik de [OpenAI Tokenizer](https://platform.openai.com/tokenizer) tool om te begrijpen hoe een stuk tekst door een taalmodel kan worden getokeniseerd en om het totale aantal tokens in dat stuk tekst te achterhalen.
:::

### Lagen in een neuraal netwerk

Een LLM bestaat uit vele lagen (layers) die de verbanden omvatten tussen de tokens:

- **Inputlaag**: zet de tokens om in getallen (vectoren) die het model kan verwerken.  
- **Verborgen lagen**: duizenden tot miljoenen rekencellen (“neuronen”) die verbanden zoeken tussen woorden. Elke laag leert andere patronen, van simpele verbanden (meervouden, grammatica) tot complexe relaties (stijl, betekenis).  
- **Outputlaag**: rekent voor alle mogelijke volgende tokens een kans uit, en kiest aan de hand van parameters welk token het meest geschikt is. 

Hoe meer lagen en parameters een model heeft, hoe verfijnder de patronen die het kan herkennen en gebruiken. 

::: {#fig-ai-lagen fig-cap="Lagen of layers van een AI-model" fig-align="center"}
```{dot}
digraph ANN {
    rankdir=LR;
    splines=line;
    node [shape=circle, fixedsize=true, width=0.6, fontsize=10];

    subgraph cluster_input {
        label="Input Layer";
        color=lightgrey;
        I1 [label="x₁"];
        I2 [label="x₂"];
        I3 [label="x₃"];
        I4 [label="x₄"];
    }

    subgraph cluster_hidden1 {
        label="Hidden Layer 1";
        color=lightblue;
        H1_1 [label="h₁₁"];
        H1_2 [label="h₁₂"];
        H1_3 [label="h₁₃"];
        H1_4 [label="h₁₄"];
    }

    subgraph cluster_hidden2 {
        label="Hidden Layer 2";
        color=lightblue;
        H2_1 [label="h₂₃"];
        H2_2 [label="h₂₁"];
        H2_3 [label="h₂₂"];
    }

    subgraph cluster_output {
        label="Output Layer";
        color=lightgreen;
        O1 [label="y₃"];
        O2 [label="y₁"];
        O3 [label="y₂"];
    }

    # Connections Input -> Hidden1
    I1 -> {H1_1 H1_2 H1_3 H1_4};
    I2 -> {H1_1 H1_2 H1_3 H1_4};
    I3 -> {H1_1 H1_2 H1_3 H1_4};
    I4 -> {H1_1 H1_2 H1_3 H1_4};

    # Connections Hidden1 -> Hidden2
    H1_1 -> {H2_1 H2_2 H2_3};
    H1_2 -> {H2_1 H2_2 H2_3};
    H1_3 -> {H2_1 H2_2 H2_3};
    H1_4 -> {H2_1 H2_2 H2_3};

    # Connections Hidden2 -> Output
    H2_1 -> {O1 O2 O3};
    H2_2 -> {O1 O2 O3};
    H2_3 -> {O1 O2 O3};
}
```
:::

::: {.callout-note title="LLM parameters"}
De belangrijkste parameters van een LLM vallen in drie groepen:

- **Architectuur**: zoals modelgrootte (aantal parameters) en contextvenster (maximale tekstlengte, van ca. 2.000 tokens in oudere modellen tot 128.000+ in moderne).  
- **Generatie**: o.a. [temperatuur](https://www.vellum.ai/llm-parameters/temperature), die de creativiteit bepaalt. Lage waarden (~0,2) geven consistente, feitelijke antwoorden; hogere (~0,8) zorgen voor meer creativiteit.  
- **Sampling**: instellingen zoals [top-k](https://www.vellum.ai/llm-parameters/top-k) en [top-p](https://www.vellum.ai/llm-parameters/top-p) sturen de woordkeuze. Bij top-k=50 kiest het model uit de 50 waarschijnlijkste woorden; bij top-p=0,9 alleen uit de woorden die samen 90% van de kansmassa vormen.  
:::

### Het wiskundige model

Een LLM werkt met vectoren en matrices: rijen en kolommen met getallen. 
Elke token wordt vertaald naar een getal, meer specifiek een vector. 
Het model gebruikt vervolgens wiskundige bewerkingen (optellen, vermenigvuldigen, transformaties) om de verbanden te berekenen. Dit heet [lineaire algebra](https://nl.wikipedia.org/wiki/Lineaire_algebra).

De achterliggende vergelijking van een neurale netwerk stelt doorgaans de output van een neuron voor als een gewogen som van de inputs plus een [bias](https://nl.wikipedia.org/wiki/Bias_(fout)), vaak geschreven als

$$
y = f\left(\sum w_i x_i + b\right)
$$

waarbij $w_i$ de gewichten zijn, $x_i$ de inputs, $b$ de bias, en $f$ een activatiefunctie.

Deze vergelijking is fundamenteel voor het begrijpen van hoe neurale netwerken informatie verwerken en leren uit data. Het netwerk voert enorme rekenstappen uit om te zoeken naar patronen. Het resultaat is een kansverdeling: welk token is het meest waarschijnlijk als volgende?

## Prompt en prompt engineering

### Prompt

Een *prompt* is de invoer die je aan een AI-model geeft: de vraag, opdracht of instructie waarmee je het model aan het werk zet. Hoe duidelijker en specifieker de prompt, hoe beter het antwoord. Je kan het vergelijken met een zoekopdracht: een vage vraag als *`Vertel iets over dieren`* levert algemene antwoorden op, terwijl *`Leg in 100 woorden uit waarom bijen belangrijk zijn voor de natuur`* een veel gerichter resultaat geeft.

### Prompt Engineering

*Prompt engineering* betekent het bewust en systematisch formuleren van prompts om betere resultaten te krijgen. Daarbij gebruik je technieken zoals:

- **Context geven**: achtergrondinformatie of details toevoegen.
- **Formaat sturen**: aangeven of je een lijst, tabel, codefragment of verhaaltje wil.
- **Voorbeelden geven**: tonen hoe een goed antwoord eruitziet (*few-shot prompting*).
- **Randvoorwaarden stellen**: bijvoorbeeld de lengte, stijl of doelgroep bepalen.

Door slim te variëren met je prompts kun je de AI sturen: van feitelijke en bondige antwoorden tot creatieve en verhalende teksten. Prompt engineering is dus het vakmanschap van vragen leren stellen die de AI het beste laten presteren. 

::: {.callout-note title="Voorbeelden"}

 -  Basis prompt:

    ```texinfo
    Schrijf een verhaaltje over een hond.
    ```

 -  Geoptimaliseerde prompt:

    ```texinfo
    Schrijf een kort verhaaltje (max. 150 woorden) over een hond die verdwaalt in een stad en door een kat naar huis wordt gebracht. Gebruik eenvoudige taal.
    ```

 -  Prompt met voorbeelden (*few-shot*):

    ```texinfo
    Voorbeeld 1:
    Vraag: Wat is de hoofdstad van Frankrijk?
    Antwoord: Parijs

    Voorbeeld 2:
    Vraag: Wat is de hoofdstad van Spanje?
    Antwoord: Madrid

    Nu jij:
    Vraag: Wat is de hoofdstad van Italië?
    Antwoord:
    ```
:::

### Stappenplan: Beter werken met prompts

1. **Bepaal je doel**  
   Denk na wat je precies van de AI wil: een uitleg, een lijstje, een verhaal, een tabel ... 

2. **Formuleer duidelijk en specifiek**  
   Vermijd vage opdrachten. Voeg trefwoorden of beperkingen toe (lengte, doelgroep, stijl).  
    -  Vage prompt:

       ```texinfo
       Vertel iets over sterren.
       ```

    -  Specifieke prompt:

       ```texinfo
       Leg in max. 100 woorden uit hoe sterren ontstaan, op een manier die een 15-jarige begrijpt.
       ```

3. **Geef context**  
   Lever extra achtergrondinfo of een situatie mee, zodat het model weet waar het zich op moet richten.

    -  Voorbeeld:

       ```texinfo
       Je bent een leraar biologie. Leg in eenvoudige taal uit hoe fotosynthese werkt.
       ```

4. **Stuur het formaat**  
   Vraag expliciet om een lijst, stappenplan, tabel of korte paragraaf.

    -  Voorbeeld:

       ```texinfo
       Geef drie voordelen van zonne-energie in tabelvorm.
       ```

5. **Gebruik voorbeelden (optioneel)**  
   Toon hoe een goed antwoord eruitziet, zodat de AI het patroon volgt (*few-shot prompting*).  

6. **Experimenteer en verbeter**  
   Pas je prompt aan als het antwoord niet helemaal goed is. Soms is één woord toevoegen of herformuleren al genoeg.
   Heb je uiteindelijk een goed resultaat, vraag dan welke prompt je direct tot dit resultaat brengt.


## Praktische tips

Hoe haal je het beste uit AI-modellen? Deze tips helpen je om snel, nauwkeurig en reproduceerbaar resultaten te krijgen – essentieel voor wetenschappelijk werk.

### Gebruik Markdown voor structuur en duidelijkheid
AI-modellen begrijpen opmaak vaak als instructie. Je invoerveld bevat geen tekstverwerker. Toch kan je programmatisch opmaak meegeven. Gebruik [Markdown](https://nl.wikipedia.org/wiki/Markdown) om je prompt visueel en functioneel te sturen:

 -  Vet of cursief voor nadruk:

    ```texinfo
    Geef een **beknopte** samenvatting (*max. 3 zinnen*) van de theorie van evolutionaire psychologie.
    ```

 -  Lijsten voor stapsgewijze output:

    ```texinfo
    Leg de wetenschappelijke methode uit in 5 stappen. Gebruik een genummerde lijst.
    ```

 -  Codeblokken voor technische precisie (bv. formules, code):

    ```texinfo
    Schrijf een Python-functie die de standaarddeviatie berekent. Gebruik dit formaat:
    """python
    import math
    def standaarddeviatie(data):
        # jouw code hier
    """
    ```

### Ken de beperkingen en sterktes van het model
Niet elk model is geschikt voor elke taak. Overweeg:

 -  Kenniscut-off
    : Modellen zoals *ChatGTP* of *Le Chat* hebben een vaste kennisdatum (bv. november 2024). Vraag niet naar actuele gebeurtenissen na die datum.

    ```texinfo
    Geef een overzicht van AI-ontwikkelingen **voor november 2024**, gericht op medische toepassingen.
    ```

 -  Specialisatie
    : Sommige modellen zijn beter in creativiteit (bv. verhalen schrijven), andere in feiten (bv. wetenschappelijke uitleg). Test welk model past bij je doel. Een indicatie van de modellen en hun sterktes.

    ::: {.callout-note title="AI modellen en relatieve sterktes" collapse="true"}
    | Model | Creativiteit (verhalen, ideeën) | Feiten & kennis | Structuur & logica | Code & data | Dialoog & ondersteuning |
    | :--- | :---: | :---: | :---: | :---: | :--- |
    | **Claude 3 Opus (Anthropic)** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ (Zeer menselijk, hoge veiligheid) |
    | **GPT-4o (OpenAI)** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ (Snel, uitstekende multimodaliteit) |
    | **Gemini 1.5 Pro (Google)** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ (Uitzonderlijk lange context van 1M+ tokens) |
    | **Llama 3.1 70B (Meta)** | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ (Krachtig, open-source leider, meertalig) |
    | **Mistral Large (Mistral AI)** | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐ |
    | **Gemini 1.5 Flash (Google)** | ⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐ (Snel, lichtgewicht, kostenefficiënt) |
    | **Mistral Codestral (Mistral AI)** | ⭐⭐ | ⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ (Gespecialiseerd voor code generatie) | ⭐⭐ |
    | **Perplexity R1 (Perplexity)** | ⭐⭐ | ⭐⭐⭐⭐⭐ (Realtime zoeken en bronvermelding) | ⭐⭐⭐ | ⭐⭐ | ⭐⭐ |
    : AI modellen en relatieve sterktes (meer sterren = sterker). {#tbl-ai-modellen-sterktes"}

    ⭐ = relatieve sterkte (meer sterren = sterker). 

    Voor **GPT-5 (OpenAI)** ontbreken momenteel publieke testresultaten. Dit model is daarom **niet opgenomen in de tabel**.  
    De tabel hierboven is samengesteld op basis van (zelf-)evaluatie en ervaring met de verschillende modellen. Gebruik de resultaten dus met de nodige relativering.
    :::

 -  Ethische grenzen
    : Vraag geen medisch advies, persoonlijke gegevensverwerking of illegale content. Gebruik bijvoorbeeld:

    ```texinfo
    Leg **algemeen** uit hoe een klinische studie werkt, zonder specifieke patiëntgegevens.
    ```

### Geef output door aan andere modellen (of tools)
AI-modellen kunnen elkaar aanvullen. Voorbeelden:

 -  Vertalen + samenvatten:

    ```texinfo
    Vertaal deze Engelse abstract naar het Nederlands, en vat hem vervolgens samen in 100 woorden.
    ```

 -  Code genereren + uitleggen:

    ```texinfo
    Schrijf een R-script voor lineaire regressie, en leg elke stap uit alsof ik een beginner ben.
    ```

 -  Combineer met tools: Gebruik AI-output als input voor Zotero (bv. voor het genereren van trefwoorden) of Excel (bv. voor datatabellen).


### Voor gevorderden
Technische tips voor gevorderden om de model paramters te beïnvloeden.

 -  Temperature & randomness
    : Laat de AI creatiever (hogere temperature, bv. 0.8) of feitelijker (lagere temperature, bv. 0.2) antwoorden. Voor wetenschappelijke teksten is 0.2–0.5 vaak ideaal.

    ```default
    Schrijf een hypothetisch onderzoeksvraag over klimaatverandering. Gebruik een creativiteitsniveau van 0.3.
    ```

 -  System prompts (voor ontwikkelaars)
    : Geef het model een rol of context vooraf:

    ```default
    Je bent een **<followup encodedFollowup="%7B%22id%22%3A%22f99430f5-dcec-4a50-975c-3e9714a4edab%22%2C%22snippet%22%3A%22wetenschappelijk%20redacteur%22%2C%22question%22%3A%22Welke%20AI-modellen%20zijn%20het%20meest%20geschikt%20voor%20het%20redigeren%20van%20wetenschappelijke%20teksten%3F%22%7D" />**. Corigeer deze alinea op helderheid en nauwkeurigheid, met bronvermeldingen in <followup encodedFollowup="%7B%22id%22%3A%22ca665769-f6fe-4109-9d5b-ff37f77cf927%22%2C%22snippet%22%3A%22APA-stijl%22%2C%22question%22%3A%22Hoe%20genereer%20ik%20correcte%20APA-vermeldingen%20met%20behulp%20van%20AI%3F%22%7D" />.
    ```
    
 -  Tokenlimieten
    : Lange prompts of antwoorden kunnen worden afgekapt. Splits complexe vragen op:

    ```default
    Vraag 1: Wat zijn de hoofdcomponenten van een onderzoeksvoorstel?
    Vraag 2: Geef een voorbeeld van een **methodologie-sectie** voor kwalitatief onderzoek.
    ```

### Ethisch en verantwoord prompten

 -  Voorkom bias
    : Vermijd stereotyperende taal. Gebruik bijvoorbeeld:

    ```default
    Beschrijf een **diverse** groep wetenschappers (geslacht, etniciteit, leeftijd) die samenwerken aan een project.
    ```

 -  Privacy
    : Vraag nooit om persoonsgegevens te verwerken. Anonimiseer voorbeelden:

    ```default
    Genereer een fictief **interviewtranscript** over studiestress, zonder echte namen of locaties.
    ```

 -  Transparantie
    : Geef aan als je AI-output gebruikt in wetenschappelijk werk. Voorbeeld:

    ```default
    Deze literatuuranalyse is **deels gegenereerd met AI** en handmatig geverifieerd op nauwkeurigheid.
    ```


### Prompt-iteratie
Een goede prompt maak je zelden in één keer. Experimenteer met deze stappen:

 -  Eerste poging:

    ```default
    Vertel over de impact van sociale media.
    ```

 -  Verbeterd met context en formaat:

    ```default
     Maak een **tabel** met 3 positieve en 3 negatieve effecten van sociale media op de **geestelijke gezondheid van adolescenten** (15–18 jaar). Gebruik **wetenschappelijke bronnen** als referentie.
    ```

 -  Optimaliseer verder:

    ```default
    Voeg aan de tabel een **korte literatuurverwijzing** toe (APA-stijl) voor elk effect, gebaseerd op studies na 2020.
    ```

### Checklist voor een sterke prompt

| Vraag | Voorbeeld |
|-------|-----------|
| Doel? | "Ik wil een stappenplan voor een systematische review." |
| Specifiek? | "Max. 7 stappen, gericht op medische literatuur." |
| Context? | "Voor een scriptie over diabetes type 2." |
| Formaat? | "Genummerde lijst met korte uitleg per stap." |
| Randvoorwaarden? | "Gebruik APA-stijl voor bronvermeldingen." |
: Checklist voor een sterke prompt. {#tbl-ai-prompt-checklist"}

## Evolutie van LLMs

De ontwikkeling van LLMs staat niet los van oudere theorieën uit de wiskunde en computerwetenschappen. De concepten en formules gaan terug tot de jaren 1950. De hedendaagse toepassing is wel nieuw.

Op 30 november 2022 lanceerde OpenAI [ChatGPT](https://en.wikipedia.org/wiki/ChatGPT).
De praktische doorbraak van Generatieve AI kwam er pas dankzij de enorme vooruitgang in rekenkracht ([CPUs](https://nl.wikipedia.org/wiki/Processor_(computer)) en vooral [GPUs](https://nl.wikipedia.org/wiki/Graphics_processing_unit)), grote datasets en cloudinfrastructuur.
Door moderne hardware konden de wiskundige concepten op grote schaal toegepast worden.

Het jaar 2022 wordt hierdoor wereldwijd beschouwd als het begin van een nieuw tijdperk waarin het niet langer zeker is of tekst en beeld door mensen of door AI zijn gemaakt [@Bellens2025b].

## Gevaren en beperkingen

AI tools bieden enorm veel voordelen en kansen, maar er schuilt ook een gevaar in. 
We zien nu al hoe AI een afnemend effect heeft op het denkvermogen van gebruikers en voornamelijk jongeren. Wie AI mag gebruiken om teksten te schrijven, hoeft zelf nauwelijks nog kritisch of creatief na te denken. Een opdracht wordt gewoon ingevoerd in ChatGPT en daar rolt een essay, gedicht of onderzoeksverslag uit — netjes opgebouwd, met cijfers en bronnen (al zijn die vaak foutief of onbetrouwbaar). Toch missen zulke teksten vaak een menselijke logica en samenhang. Het zijn vooral losse woorden die volgens de instructies in de juiste volgorde zijn gezet, zonder echte natuurlijke flow.

::: {.callout-warning title="Onverantwoord gebruik maakt dommer"}
Onderzoekers zijn bezorgd voor de cognitieve impact op de gebruikers [@vanleemputten2025]. Een onderzoek van MIT toont hoe ons brein minder verbindingen maakt wanneer we steunen op AI-tools zoals ChatGPT [@kosmyna2025]. De meeste testpersonen kunnen zelfs niet navertellen wat ze kort voorheen hadden geschreven. Onverantwoord AI gebruiken maakt de gebruikers dus dommer.
::: 

## Hallucineren of confabuleren?

Omdat een LLM alleen leert wat waarschijnlijk klinkt en geen ingebouwd systeem heeft om waarheid van onwaarheid te onderscheiden, kan het *hallucineren*: overtuigende maar onjuiste of verzonnen informatie geven. 
Onderzoekers stellen voor dit eigenlijk eerder *confabulatie* te noemen: er is geen sprake van opzet, en het model is zich niet bewust van het feit dat de output niet klopt. Het model “moet” steeds een volgend token kiezen, ook als er geen duidelijke of betrouwbare optie bestaat.

In september 2025 bevestigde OpenAI dit in hun rapport *Why Language Models Hallucinate* [@kalai2025]. Hun kernbevindingen:

- **Hallucinaties zijn inherent en wiskundig** aan de huidige aanpak van *next token prediction*. Zelfs met perfecte trainingsdata zijn fouten onvermijdelijk.  
- **Evaluatie en training belonen gokken**: modellen worden beloond voor het geven van antwoorden, niet voor het toegeven van onzekerheid.  
- **Singleton facts zijn kwetsbaar**: feiten die maar één keer in de training voorkomen, worden vaak fout gereproduceerd.  
- **Finetuning helpt maar lost het probleem niet volledig op**: omdat benchmarks en scoringssystemen vooral zekerheid belonen.  

De onderzoekers benadrukken dat betere evaluaties nodig zijn: modellen zouden beloond moeten worden voor het correct aangeven van onzekerheid of “ik weet het niet”, in plaats van voor foutieve maar overtuigende antwoorden.

## AI als hulpmiddel bij onderzoek

Naast de theoretische en technische achtergrond is het belangrijk te benadrukken dat AI in onderzoek een ondersteunende rol kan spelen. 

### Ondersteuning

AI-modellen zoals LLMs zijn geen vervanging voor kritisch denken of wetenschappelijke methodiek, maar kunnen wel tijd besparen en nieuwe invalshoeken aanreiken. Enkele voorbeelden:

- Zoeksyntaxis en databanken
  : AI kan helpen om zoekopdrachten te verfijnen, synoniemen of relevante trefwoorden aan te reiken, en complexe zoeksyntaxis voor academische databanken (bv. Web of Science, PubMed) correct op te stellen.
- Herformuleren van teksten
  : studenten kunnen AI gebruiken om een eerste ruwe tekst in een betere academische stijl te herschrijven, of om lange passages samen te vatten.
- Brainstorm en ideeëngeneratie
  : AI kan mogelijke invalshoeken of vragen voorstellen die anders misschien over het hoofd gezien worden.
- Taalondersteuning
  : AI kan bijdragen aan vertalingen, spellingcontrole en het vereenvoudigen van vakjargon om het toegankelijker te maken.

### Kritische kanttekening

Het gebruik van AI in onderzoek vraagt altijd om menselijke controle:

- AI-output moet worden nagekeken op juistheid en betrouwbaarheid.  
- De gebruiker blijft verantwoordelijk voor bronvermelding en ethisch gebruik.  
- Ongecontroleerd vertrouwen op AI kan leiden tot fouten of plagiaat.  

In de context van wetenschappelijke methodiek kan AI dus vooral dienen als ondersteunende tool: het versnelt bepaalde taken en kan creatief inspireren, maar de kwaliteit van onderzoek staat of valt met de kritische blik van de onderzoeker zelf.

### Verantwoord gebruik

Het inzetten van AI bij onderzoek kan zeer nuttig zijn, maar vraagt om bewuste keuzes. Enkele richtlijnen:

1. Zie AI als hulpmiddel, niet als vervanger
   : Gebruik AI om ideeën te verkennen, teksten te herformuleren of zoekopdrachten te verbeteren, maar behoud altijd je eigen kritische blik.

2. Controleer altijd de inhoud
   : AI kan foutieve of verzonnen informatie (“confabulatie”) geven. Controleer dus steeds bij betrouwbare bronnen of de output klopt.

3. Wees transparant
   : Als AI een rol heeft gespeeld in je werk (bv. bij het formuleren van zoektermen of het samenvatten van literatuur), vermeld dit kort. Dit verhoogt de betrouwbaarheid en voorkomt misverstanden.

4. Gebruik AI om je eigen vaardigheden te versterken
   : Laat AI suggesties doen voor formuleringen, maar probeer altijd zelf de kern te verwoorden. Zo ontwikkel je je academische schrijf- en denkvaardigheden.

5. Beperk copy-paste gedrag
   : AI-output mag nooit één-op-één overgenomen worden zonder na te denken. Gebruik AI-resultaten als inspiratie of startpunt, niet als eindproduct.

6. Let op ethiek en plagiaat
   : Het is niet toegestaan om AI-gegenereerde teksten zonder bronvermelding als eigen werk te presenteren. Combineer AI-output altijd met eigen inzichten en correcte citaties.

7. Hou rekening met duurzaamheid
   : AI-verzoeken kosten veel rekenkracht en dus energie, maar ook water. Denk na over de noodzaak van elke query en gebruik AI bewust.

AI kan een krachtige assistent zijn in het onderzoeksproces, zolang je kritisch blijft, correct omgaat met bronvermelding, en beseft dat jijzelf de onderzoeker bent – niet de tool.

::: {.callout-warning title="Duurzaamheid"}
Een enkel ChatGPT-verzoek verbruikt tien keer meer elektriciteit dan een Google- zoekopdracht [@UNWE2025]. AI-toepassingen draaien voornamelijk in energie-intensieve datacenters waarvan het elektriciteitsverbruik naar schatting is gestegen tot ongeveer 415 TWh in 2024 (~1,5 % van het mondiale elektriciteitsverbruik), en dit zal naar verwachting verdubbelen tot circa 945 TWh in 2030 (~3 %) [@IAE2025]. Veruit de meeste energie kruipt in het trainen van het model. 
Tegelijk verbruikt een sessie van 10 tot 50 ChatGPT-vragen ongeveer een halve liter water [@li2025], voornamelijk voor de koeling van servers. Het mondiale waterverbruik door datacenters kan in 2030 oplopen tot miljarden kubieke meters, wat neerkomt op het waterverbruik van hele landen. 

Deze snelle groei wordt gedreven door zogenaamde “accelerated servers” voor AI, waarvan het elektriciteitsverbruik jaarlijks met ongeveer 30 % toeneemt, tegen zo'n 9 % voor reguliere servers. De productie van chips voor AI draagt daarnaast ook aan het waterverbruik bij.

{{< video https://youtu.be/DGjj7wDYaiI?si=EyVU5hadV6czoN5s >}}
:::


::: {.callout-warning title="Controversies"}
Het belangrijkste product van Perplexity is een zoekmachine die wordt aangedreven door generatieve AI, die online en binnen interne documenten naar informatie zoekt en deze samenvat om de vraag van de gebruiker te beantwoorden. In vakjargon een *RAG* ofwel *retrieval-augmented generation* techniek wat LLMs voorziet in het ophalen en verwerken van nieuwe informatie. Heel handig, zou je denken. Van alle AI-tools is Perplexity het beste in het zoeken op het internet en voorleggen van referenties waaronder tijdschriften en boeken. Maar hoe is Perplexity aan deze bronnen en kennis geraakt?

Perplexity is meermaals beschuldigd van inbreuk op auteursrechten en handelsmerken [@Nikkei2025_ai;@BritannicaMerriamWebster2025_ai]. 
Er is ook aangetoond dat Perplexity gebruik maakt van heimelijke, niet-aangegeven crawlers om no-crawl-richtlijnen van websites te omzeilen [@Cloudflare2025_ai].

Perplexity is lang niet de enige. Alleen al in 2025 lopen 77 rechtzaken omtrent het schenden van auteursrechtelijke kwesties met betrekking tot het creëren en gebruiken van generatieve AI [@Panettieri2025_ai; @Weisenberger2025_ai].
Gen-AI heeft zijn voordelen, maar ben je er van bewust dat modellen altijd getraind en gevoed moeten worden met kennis. Stel dus voldoende in vraag waar die kennis vandaan komt.
:::


::: {.callout-note title="Voorkom dat je gesprekken worden gebruikt voor training"}

#### ChatGPT
1. Ga naar [https://chat.openai.com](https://chat.openai.com)
2. Klik linksonderaan op je account en vervolgens op **"Instellingen"**.
3. Ga naar **"Data controls"**.
4. Schakel de schakelaar uit bij:
   *"Verbeter het model voor iedereen"*

#### Gemini
1. Ga naar [https://myactivity.google.com/product/gemini](https://myactivity.google.com/product/gemini)
2. Klik op **"Gemini-appsactiviteit"**.
3. Schakel de schakelaar uit bij:
   *"Je Gemini-appsactiviteit opslaan in je Google-account."*

#### Le Chat
1. Ga naar [https://chat.mistral.ai](https://chat.mistral.ai)
2. Klik linksbovenaan op je account en vervolgens op **"Instellingen"**.
3. Scroll naar beneden en klik links onderaan bij *Le Chat* op **"Voorkeuren"**.
4. Schakel de schakelaar uit bij:
   *"Sta toe dat uw interacties worden gebruikt om onze modellen te trainen."*
:::
